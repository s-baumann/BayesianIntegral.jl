var documenterSearchIndex = {"docs":
[{"location":"3_api_reference/#3.0-API-Reference","page":"3.0 API Reference","title":"3.0 API Reference","text":"","category":"section"},{"location":"3_api_reference/#BayesianIntegral.GaussianKernelHyperparameters","page":"3.0 API Reference","title":"BayesianIntegral.GaussianKernelHyperparameters","text":"GaussianKernelHyperparameters\n\nThis contains the hyperparameters for the gaussian kernel. The functional form for correlation between xp and xq   is   w0 exp( -0.5 * \\sum{d=1}^D ((x{p,d} - x{q,d})/wd)^2)     where D is the number of dimensions. and each term of the summation is a different dimension of xp and x_q.\n\n\n\n\n\n","category":"type"},{"location":"3_api_reference/#BayesianIntegral.RProp_params","page":"3.0 API Reference","title":"BayesianIntegral.RProp_params","text":"RProp_params\n\n\n\n\n\n","category":"type"},{"location":"3_api_reference/#BayesianIntegral.K_matrix-Union{Tuple{R}, Tuple{AbstractMatrix{R}, Any, GaussianKernelHyperparameters}, Tuple{AbstractMatrix{R}, Any, GaussianKernelHyperparameters, Real}} where R<:Real","page":"3.0 API Reference","title":"BayesianIntegral.K_matrix","text":"K_matrix(X::Array{Float64,2}, cov_func::Function, cov_func_parameters::GaussianKernelHyperparameters, noise::Float64 = 0.0)\n\nReturns a Kmatrix together with marginal K matrices (marginal over each hyperparameter). THe covfunc should be a function with a signature like that of gaussian_kernel.\n\n\n\n\n\n","category":"method"},{"location":"3_api_reference/#BayesianIntegral.K_matrix_with_marginals-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Any, GaussianKernelHyperparameters}, Tuple{AbstractMatrix{T}, Any, GaussianKernelHyperparameters, Real}} where T<:Real","page":"3.0 API Reference","title":"BayesianIntegral.K_matrix_with_marginals","text":"K_matrix_with_marginals(X::Array{Float64,2}, cov_func::Function, cov_func_parameters::GaussianKernelHyperparameters, noise::Float64 = 0.0)\n\nReturns a K_matrix together with marginal K matrices (marginal over each hyperparameter)\n\n\n\n\n\n","category":"method"},{"location":"3_api_reference/#BayesianIntegral.calibrate_by_ML_with_Rprop-Union{Tuple{R}, Tuple{AbstractMatrix{<:Real}, AbstractVector{R}, GaussianKernelHyperparameters, Integer, Real, RProp_params}} where R<:Real","page":"3.0 API Reference","title":"BayesianIntegral.calibrate_by_ML_with_Rprop","text":"calibrate_by_ML_with_Rprop(X, y, cov_func_parameters, MaxIter, noise, params)\n\nTrains kriging hyperparameters by maximising marginal likelihood with RProp.\n\n\n\n\n\n","category":"method"},{"location":"3_api_reference/#BayesianIntegral.calibrate_by_ML_with_SGD-Union{Tuple{R}, Tuple{AbstractMatrix{<:Real}, AbstractVector{R}, GaussianKernelHyperparameters, Integer, Integer}, Tuple{AbstractMatrix{<:Real}, AbstractVector{R}, GaussianKernelHyperparameters, Integer, Integer, Real}, Tuple{AbstractMatrix{<:Real}, AbstractVector{R}, GaussianKernelHyperparameters, Integer, Integer, Real, Real}, Tuple{AbstractMatrix{<:Real}, AbstractVector{R}, GaussianKernelHyperparameters, Integer, Integer, Real, Real, Random.MersenneTwister}} where R<:Real","page":"3.0 API Reference","title":"BayesianIntegral.calibrate_by_ML_with_SGD","text":"calibrate_by_ML_with_SGD(X, y, cov_func_parameters, steps, batch_size, step_multiple, noise, twister)\n\nTrains kriging hyperparameters by maximising marginal likelihood with stochastic gradient descent.\n\n\n\n\n\n","category":"method"},{"location":"3_api_reference/#BayesianIntegral.correlation_vector_of_a_point-Union{Tuple{T}, Tuple{R}, Tuple{AbstractVector{T}, AbstractMatrix{R}, Any, GaussianKernelHyperparameters}} where {R<:Real, T<:Real}","page":"3.0 API Reference","title":"BayesianIntegral.correlation_vector_of_a_point","text":"correlation_vector_of_a_point(x::AbstractArray{T,1},X::AbstractArray{R,2}, cov_func::Function, cov_func_parameters::GaussianKernelHyperparameters) where T<:Real where R<:Real\n\nThis calculates the correlations of a point with each point in the X array.\n\n\n\n\n\n","category":"method"},{"location":"3_api_reference/#BayesianIntegral.evaluate-Union{Tuple{T}, Tuple{KrigingModel, AbstractVector{T}}} where T<:Real","page":"3.0 API Reference","title":"BayesianIntegral.evaluate","text":"evaluate(modl::KrigingModel, PointToExamine::AbstractArray{T,1}) where T<:Real\n\nReturns the kriging predictor value (ordinary kriging estimate) at the given point.\n\n\n\n\n\n","category":"method"},{"location":"3_api_reference/#BayesianIntegral.expected_improvement-Tuple{KrigingModel, Real, AbstractVector{<:Real}}","page":"3.0 API Reference","title":"BayesianIntegral.expected_improvement","text":"expected_improvement(modl::KrigingModel, fmin::Real, PointToExamine::AbstractArray{<:Real,1})\n\nComputes the expected improvement (Jones, Schonlau, Welch Equation 15) at a point. Returns how much improvement over the current best value fmin is expected.\n\n\n\n\n\n","category":"method"},{"location":"3_api_reference/#BayesianIntegral.gaussian_kernel-Union{Tuple{T}, Tuple{R}, Tuple{AbstractVector{T}, AbstractVector{R}, GaussianKernelHyperparameters}} where {R<:Real, T<:Real}","page":"3.0 API Reference","title":"BayesianIntegral.gaussian_kernel","text":"gaussian_kernel(x1::Array{Float64,1}, x2::Array{Float64,1}, cov_func_parameters::GaussianKernelHyperparameters)\n\nReturns a covariance estimated with a gaussian kernel.\n\n\n\n\n\n","category":"method"},{"location":"3_api_reference/#BayesianIntegral.get_next_query_point_through_expected_improvement","page":"3.0 API Reference","title":"BayesianIntegral.get_next_query_point_through_expected_improvement","text":"get_next_query_point_through_expected_improvement(modl::KrigingModel, lower, upper)\n\nFinds the point that maximises expected improvement over the current best observed value. Uses SAMIN optimisation over the box [lower, upper].\n\n\n\n\n\n","category":"function"},{"location":"3_api_reference/#BayesianIntegral.get_predicted_minimum","page":"3.0 API Reference","title":"BayesianIntegral.get_predicted_minimum","text":"get_predicted_minimum(modl::KrigingModel, lower, upper)\n\nFinds the predicted minimum of the kriging model over the box [lower, upper]. Uses SAMIN optimisation.\n\n\n\n\n\n","category":"function"},{"location":"3_api_reference/#BayesianIntegral.integrate-Union{Tuple{U}, Tuple{KrigingModel, AbstractVector{U}, LinearAlgebra.Hermitian}} where U<:Real","page":"3.0 API Reference","title":"BayesianIntegral.integrate","text":"integrate(modl::KrigingModel, prob_means::AbstractArray{U,1}, covar::Hermitian) where U<:Real\n\nReturns the expectation and variance of the integral of a kriging model given the probabilities described by a multivariate normal with means (in each dimension) of probmeans and covariance matrix covar. The integration performed is: int{x in X} f(x) p(x) dx Where f(x) is the function which is approximated in the kriging map by an exponential covariance function and p(x) is the pdf which is multivariate gaussian.\n\n\n\n\n\n","category":"method"},{"location":"3_api_reference/#BayesianIntegral.log_likelihood-Union{Tuple{T}, Tuple{AbstractVector{T}, LinearAlgebra.Hermitian}} where T<:Real","page":"3.0 API Reference","title":"BayesianIntegral.log_likelihood","text":"log_likelihood( y::Array{Float64,1},  K::Hermitian{Float64,Array{Float64,2}}; invK::Hermitian{Float64,Array{Float64,2}} = inv(K), determinant = det(K))\n\nThe log likelihood of a kriging model with values y and covariances K. invK and the determinant can be fed in as well to prevent additional operations. Note that the normalising constant is excluded from the log likelihood here because it is not relevent for optimising hyperparameters.\n\n\n\n\n\n","category":"method"},{"location":"3_api_reference/#BayesianIntegral.marginal_gaussian_kernel-Union{Tuple{T}, Tuple{R}, Tuple{AbstractVector{T}, AbstractVector{R}, GaussianKernelHyperparameters}} where {R<:Real, T<:Real}","page":"3.0 API Reference","title":"BayesianIntegral.marginal_gaussian_kernel","text":"marginal_gaussian_kernel(x1::Array{Float64}, x2::Array{Float64}, cov_func_parameters::GaussianKernelHyperparameters)\n\nReturns a covariance estimated with a gaussian kernel. Also returns the marginal covariances (how does each covariance change by bumping each hyperparameter).\n\n\n\n\n\n","category":"method"},{"location":"3_api_reference/#BayesianIntegral.marginal_likelihood_gaussian_derivatives-Union{Tuple{T}, Tuple{R}, Tuple{AbstractMatrix{T}, AbstractVector{R}, GaussianKernelHyperparameters}, Tuple{AbstractMatrix{T}, AbstractVector{R}, GaussianKernelHyperparameters, Real}} where {R<:Real, T<:Real}","page":"3.0 API Reference","title":"BayesianIntegral.marginal_likelihood_gaussian_derivatives","text":"marginal_likelihood_gaussian_derivatives(X::Array{Float64,2}, y::Array{Float64,1}, w_0::Float64, w_i::Array{Float64,1}, noise::Float64 = 0.0)\n\nThe marginal likelihoods (along each parameter) of a kriging model are returned. In addition the K matrix and the inverse K matrix are returned (to allow programers to use them as generated here and no redo them).\n\n\n\n\n\n","category":"method"},{"location":"3_api_reference/#BayesianIntegral.predicted_error-Union{Tuple{T}, Tuple{KrigingModel, AbstractVector{T}}} where T<:Real","page":"3.0 API Reference","title":"BayesianIntegral.predicted_error","text":"predicted_error(modl::KrigingModel, PointToExamine::AbstractArray{T,1}) where T<:Real\n\nReturns the predicted standard error (square root of kriging variance) at the given point. Uses Equation 9 of Jones, Schonlau, Welch with ordinary kriging correction.\n\n\n\n\n\n","category":"method"},{"location":"3_api_reference/#BayesianIntegral.sample-Tuple{Random.MersenneTwister, Integer, Integer}","page":"3.0 API Reference","title":"BayesianIntegral.sample","text":"sample(twister::MersenneTwister, dim::Integer, batch_size::Integer)\n\nThis does sampling with or without replacement.\n\n\n\n\n\n","category":"method"},{"location":"1_bayesian_integration/#1.0-Bayesian-Integration","page":"1.0 Bayesian Integration","title":"1.0 Bayesian Integration","text":"This package implements Bayesian Integration as described  by Rasmussen & Ghahramani (2003) and before that by O'Hagan (1991). These both use Kriging techniques to map out a function. The function is then integrated using this kriging map together with a multivariate Gaussian distribution gives a mass at each point in the function.\n\nAt present only an exponential kernel is supported and only a multivariate Gaussian distribution for assigning mass to various points in the function. Thus the integrate function is the only integration function in the package. The exponential kernel used is:\n\ntextCov(f(x^p) f(x^q))=w_0e^-frac12(sum_i=1^dfracx^p_i - x_i^qw_i)^2\n\nWhere d is the dimensionality of the space the points x^p and x^q are defined in. w_0 and w_i are hyperparameters which need to be input. This is done in the GaussianKernelHyperparameters structure. These hyperparameters can be trained with the functions in the next section of the documentation. For simplicity however we have all parameters being 1.0 in the example below:\n\nusing BayesianIntegral\nusing LinearAlgebra\nusing Sobol\nsamples = 25\ndims = 2\ns = SobolSeq(dims) # We use Sobol numbers to choose where to sample but we could choose any points.\nX = convert( Array{Float64}, hcat([next!(s, repeat([0.5] , outer = dims)     ) for i = 1:samples]...)' )\nfunction func(X::Array{Float64,1})\n    return sum(X) - prod(X)\nend\ny = Array{Float64,1}(undef,samples)\nfor i in 1:samples\n    y[i] = func(X[i,:])\nend\n# We need hyperparameters which describe what covariance exists in function values across every dimension.\ncov_func_parameters = GaussianKernelHyperparameters(1.0, repeat([10.0] , outer = dims))\n# Build a kriging model from the sample points and function values.\nnoise = 0.001\nmodl = KrigingModel(X, y, cov_func_parameters; noise = noise)\n# Now we create a vector of means and a covariance matrix for the multivariate normal distribution describing the\n# probability mass at each point in the function.\nprob_means = repeat([0.0] , outer = dims)\ncovar = Hermitian(diagm(0 => ones(dims)))\n# Now finding the integral\ninteg = integrate(modl, prob_means, covar)\n\nThe result integ is a named tuple with expectation and variance fields representing a Gaussian distribution over probable integral values.","category":"section"},{"location":"2_training_hyperparameters/#2.0-Training-Hyperparameters","page":"2.0 Training Hyperparameters","title":"2.0 Training Hyperparameters","text":"This package currently supports two ways of training hyperparameters. The first is Maximum likelihood with stochastic gradient descent while the second uses a momentum based algorithm to maximise likelihood (Blum and Riedmiller 2013).","category":"section"},{"location":"2_training_hyperparameters/#2.1-Maximum-Likelihood-with-Stochastic-Gradient-Descent","page":"2.0 Training Hyperparameters","title":"2.1 Maximum Likelihood with Stochastic Gradient Descent","text":"This is done by the calibrate_by_ML_with_SGD function. The procedure is:\n\nExtract a sample of the requested size from the dataset. Sampling is done without replacement (or else the K matrix is singular and not invertible)\nFind the likelihood of the dataset given the input hyperparameters. Also find the marginal likelihood (with respect to all hyperparameters) and use the Newtonian method to suggest another set of hyperparameters. The step to the new parameter set (as chosen by the Newtonian method) can be adjusted by the step_multiple parameter.\n\nThis process continues for a user-specifiable number of iterates.\n\nNote that using stochastic gradient descent is important here as the major time here is in inverting an NxN matrix which has a complexity of about O(N^23). Thus if only 10% of observations are used in each iterate this makes the calibration more than 100 times faster than using all observations.\n\nThis function can be used in the following way:\n\nold_cov_func_parameters = GaussianKernelHyperparameters(1.0, repeat([10.0] , outer = dims))\nsteps = 100                           # How many optimisation steps\nbatch_size = 5                        # Number of observations per sample\nstep_multiple = 1.0                   # How far to step\nnoise = 0.001                         # Noise parameter\ntwister = MersenneTwister(2)          # Random number generator\nnew_cov_func_parameters = calibrate_by_ML_with_SGD(X, y, old_cov_func_parameters, steps, batch_size, step_multiple, noise, twister)","category":"section"},{"location":"2_training_hyperparameters/#2.2-RProp","page":"2.0 Training Hyperparameters","title":"2.2 RProp","text":"This technique also maximises Maximum Likelihood. The main difference is that rather than taking Newtonian steps towards an maximum, the Rprop algorithm ignores the magnitude of the marginal likelihood and looks only at the sign. It moves a small distance uphill towards a higher likelihood. If the slope is still positive at this new point it will speed up and step by a greater amount in this direction. If the sign of the gradient ever changes it will slow down. It is thus a momentum based optimiser.\n\nThis is implemented with the calibrate_by_ML_with_Rprop function as below:\n\nold_cov_func_parameters = GaussianKernelHyperparameters(1.0, repeat([10.0] , outer = dims))\nparams  = RProp_params()                             # parameters for the RProp algorithm.\nnoise = 0.001                                        # Noise parameter\nMaxIter = 2000                                       # Number of steps\nnew_cov_func_parameters = calibrate_by_ML_with_Rprop(X, y, old_cov_func_parameters, MaxIter, noise, params)","category":"section"},{"location":"99_refs/#References","page":"References","title":"References","text":"Blum, M. and Riedmiller, M. 2013. \"Optimization of Gaussian Process Hyperparameters using Rprop\". European Symposium on Artificial Neural Networks.\n\nJones, D. and Schonlau, M. and Welch, W. 1998. \"Efficient Global Optimization of Expensive Black-Box Functions\". Journal of Global Optimization 13. 455-492.\n\nO'Hagan, A. 1991. \"Bayes-Hermite Quadrature\". Journal of Statistical Planning and Inference 29 : 245-260.\n\nRasmussen, Carl, and Ghahramani, Zoubin. 2003. \"Bayesian Monte Carlo\". Advances in Neural Information Processing Systems.","category":"section"},{"location":"#BayesianIntegral","page":"BayesianIntegral","title":"BayesianIntegral","text":"This does Bayesian integration of functions of the form:\n\nint_x in Re^d f(x) g(x)\n\nWhere d is the dimensionality of the space (so x is d dimensional), f(x) is the function of interest and g(x) is a pdf representing the density of each x value.\n\nThis package uses the term Bayesian Integration to mean approximating a function with a kriging metamodel (aka a gaussian process model) and then integrating under it. A kriging metamodel has the nice feature that uncertainty about the nature of the function is explicitly modelled (unlike for instance a approximation with Chebyshev polynomials) and the Bayesian Integral uses this feature to give a Gaussian distribution representing the probabilities of various integral values. The output of the integrate function is the expectation and variance of this distribution.","category":"section"}]
}
